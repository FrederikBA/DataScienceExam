{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb756a9",
   "metadata": {},
   "source": [
    "# Version 1: Movie Recommender based on summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# some important packages\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199b073",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "- Now we will use Google's Universal Sentence Encoder which can generate embeddings for any sentence, those embeddings we can use to create a recommendation system for our movies dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(model_url)\n",
    "\n",
    "def embed(texts):\n",
    "    return model(texts)\n",
    "\n",
    "embed(['This movie was great!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62cb14",
   "metadata": {},
   "source": [
    "## Loading our movies csv into our dataframe.\n",
    "- In this section, we load the dataset into a pandas dataframe and select the important columns we need for this movie recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_cleaned.csv\")\n",
    "df = df[[\"title\", \"genre\", \"summary\", \"directors\", \"actors\"]]\n",
    "\n",
    "summaries = list(df['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f760e",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "- Here, we generate embeddings for each summary using the Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b69263",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(summaries)\n",
    "print('The embedding shape is:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3d2f3",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "- We use PCA to reduce the embeddings' dimensionality to 2D and plot them for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71111f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.title('Embedding space')\n",
    "plt.scatter(emb_2d[:, 0], emb_2d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e74aa",
   "metadata": {},
   "source": [
    "## Nearest Neighbors\n",
    "- We use the NearestNeighbors algorithm to find the closest movies in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b72d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82240452",
   "metadata": {},
   "source": [
    "## Recommend Function\n",
    "- We define the recommend() function that takes a text input, finds the closest movies based on their embeddings, and returns the titles of the recommended movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff98d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(text):\n",
    "    emb = embed([text])\n",
    "    neighbors = nn.kneighbors(emb, return_distance=False)[0]\n",
    "    return df['title'].iloc[neighbors].tolist()\n",
    "\n",
    "print('Recommended Movies:')\n",
    "recommend(\"After the devastating events of Avengers: Infinity War (2018), the universe is in ruins. With the help of remaining allies, the Avengers assemble once more in order to reverse Thanos' actions and restore balance to the universe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5ef77",
   "metadata": {},
   "source": [
    "# Version 2: Movie Recommender with Multiple Parameters\n",
    "\n",
    "**1. Select relevant AI methods that could solve the problem. Train, test and validate data models by using supervised and unsupervised methods, neural networks or graphs.**\n",
    "- We have chosen the BERT (Bidirectional Encoder Representations from Transformers) model, which is a state-of-the-art method for natural language processing. We've used a pre-trained BERT model to generate embeddings for combined features (title, genre, summary, directors, and actors) of the movies.\n",
    "\n",
    "**2. Select and apply appropriate measures for assessing the quality of your models. Iterate the process to explore possibilities for improving the quality of the models.**\n",
    "- Although we have not implemented an explicit quality assessment in the code, we have utilized the NearestNeighbors algorithm from the scikit-learn library to find similar movies based on the embeddings. This implicitly evaluates the quality of the embeddings, as the recommendations would not be relevant if the embeddings were of poor quality.\n",
    "\n",
    "**3. Implement the modules in the intended scenario as an AI prototype of your solution.**\n",
    "- We have implemented a movie recommender system as an AI prototype solution. The code takes a text input (movie summary), processes it with the pre-trained BERT model, and recommends similar movies using the NearestNeighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6755680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc554fa",
   "metadata": {},
   "source": [
    "### Loading our dataset and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cec7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_cleaned.csv\")\n",
    "df = df[[\"title\", \"genre\", \"summary\", \"directors\", \"actors\"]]\n",
    "df['combined'] = df['title'] + ' ' + df['genre'] + ' ' + df['summary'] + ' ' + df['directors'] + ' ' + df['actors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2178ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e28eae",
   "metadata": {},
   "source": [
    "### Loading a pre-trained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c250b",
   "metadata": {},
   "source": [
    "### Function to get sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ef7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for dataset in batches\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_embeddings(sentences, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    num_batches = (len(sentences) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), total=num_batches, desc=\"Generating embeddings\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        batch_embeddings = get_sentence_embeddings(batch, tokenizer, model)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4c281",
   "metadata": {},
   "source": [
    "### Generate embeddings for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab747df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['combined'].tolist()\n",
    "embeddings = generate_embeddings(sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd14fbc",
   "metadata": {},
   "source": [
    "### nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "# nn = NearestNeighbors(n_neighbors=10, metric=\"cosine\")\n",
    "# nn = NearestNeighbors(n_neighbors=10, metric=\"manhattan\")\n",
    "# nn = NearestNeighbors(n_neighbors=10, metric=\"minkowski\")\n",
    "nn.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac47c83",
   "metadata": {},
   "source": [
    "### Recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(text, tokenizer, model, nn, df):\n",
    "    emb = get_sentence_embeddings([text], tokenizer, model)\n",
    "    neighbors = nn.kneighbors(emb, return_distance=False)[0]\n",
    "    return df['title'].iloc[neighbors].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdf0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"After the devastating events of Avengers: Infinity War (2018), the universe is in ruins. With the help of remaining allies, the Avengers assemble once more in order to reverse Thanos' actions and restore balance to the universe.\"\n",
    "\n",
    "print('Recommended Movies:')\n",
    "print(recommend(input_text, tokenizer, model, nn, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3e851",
   "metadata": {},
   "source": [
    "### Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"movie_recommender_model\")\n",
    "tokenizer.save_pretrained(\"movie_recommender_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5db359",
   "metadata": {},
   "source": [
    "### Load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66842440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"movie_recommender_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"movie_recommender_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b47f11",
   "metadata": {},
   "source": [
    "### then we split the data into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f555fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9798cee",
   "metadata": {},
   "source": [
    "### Tokenize the data and create PyTorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71460a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_df['combined'].tolist(), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_df['combined'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = MovieDataset(train_encodings)\n",
    "valid_dataset = MovieDataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcabeb5",
   "metadata": {},
   "source": [
    "### Fine Tuning the BERT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_movie_recommender_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_movie_recommender_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6da3a",
   "metadata": {},
   "source": [
    "### Replacing the pre-trained BERT model with the fine-tuned model in our recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"fine_tuned_movie_recommender_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_movie_recommender_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"After the devastating events of Avengers: Infinity War (2018), the universe is in ruins. With the help of remaining allies, the Avengers assemble once more in order to reverse Thanos' actions and restore balance to the universe.\"\n",
    "\n",
    "print('Recommended Movies:')\n",
    "print(recommend(input_text, tokenizer, model, nn, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3af875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom holland\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenize the text and remove stop words and punctuation\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_digit]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_processed = (\" \").join(preprocess_text(\"I would like an Action movie set in a jungle starring Tom Holland as Nathan Drake\"))\n",
    "doc = nlp(sample_processed)\n",
    "\n",
    "found_actors = []\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text)\n",
    "    if(entity.label_ == \"PERSON\"): \n",
    "        found_actors.append(entity.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32892bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.9/12.8 MB 29.4 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 16.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.1/12.8 MB 22.4 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.6/12.8 MB 24.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 30.0 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 28.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 29.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.6/12.8 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.6/12.8 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\12301\\anaconda3\\envs\\school\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b1c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
